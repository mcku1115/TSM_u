tsm_u <- filter(sentence_tsm, str_detect(例句, '有'))
tsm_u
lemma_tsm <- read_csv("詞目總檔.csv")
lemma_tsm
lemma_tsm <- read_csv("詞目總檔.csv") %>% .$詞目
dict_tsm <- writeLines(lemma_tsm, 'TSM_dict.txt')
remove(dict_tsm)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(readr)
library(stringr)
lemma_tsm <- read_csv("詞目總檔.csv") %>% .$詞目
writeLines(lemma_tsm, 'TSM_dict.txt')
sentence_tsm <- read_csv("例句.csv") %>%
select(c(1, 5, 6, 7)) %>%
rename(編號 = 例句編號, 標音 = 例句標音)
head(sentence_tsm)
tsm_u <- filter(sentence_tsm, str_detect(例句, '有'))
head(tsm_u)
library(jiebaR)
segmenter <- worker(type = "tag",
user = "TSM_dict.txt",
symbol = T,
bylines = F)
# define own function
tag_text <- function(x, jiebar){
segment(x, jiebar) %>%
paste(names(.), sep = "/", collapse = " ")
}
tsm_u_tagged <- tsm_u %>%
mutate(斷詞 = map_chr(例句, tag_text, segmenter))
library(tidyverse)
library(tidyverse)
tsm_u_tagged <- tsm_u %>%
mutate(斷詞 = map_chr(例句, tag_text, segmenter))
tsm_u_tagged
sentence_tsm_tagged <- sentence_tsm %>%
mutate(斷詞 = map_chr(例句, tag_text, segmenter))
tsm_u <- filter(sentence_tsm, str_detect(例句, '\\b有/'))
head(tsm_u)
tsm_u <- filter(sentence_tsm, str_detect(斷詞, '\\b有/'))
tsm_u <- filter(sentence_tsm_tagged, str_detect(斷詞, '\\b有/'))
head(tsm_u)
tsm_u
library(quanteda)
View(tsm_u)
corpus_tsm_u
corpus_tsm_u <- corpus(tsm_u,
docid_field = '編號',
text_field = '斷詞')
corpus_tsm_u
kwic(corpus_tsm_u, '有/V', valuetype = 'regex') %>% head(10) %>% knitr::kable(align = 'c')
kwic(corpus_tsm_u, '有/v', valuetype = 'regex') %>% head(10) %>% knitr::kable(align = 'c')
corpus_tsm_u <- corpus(tsm_u,
docid_field = '編號',
text_field = '斷詞')
kwic(corpus_tsm_u, '有/v', valuetype = 'regex') %>% head(10) %>% knitr::kable(align = 'c')
kwic(corpus_tsm_u, '有/V', valuetype = 'regex') %>% head(10) %>% knitr::kable(align = 'c')
tsm_u$斷詞
tsm_u$例句
kwic(corpus_tsm_u, '有/V') %>% head(10) %>% knitr::kable(align = 'c')
corpus_tsm_u
tsm_u_tokens <- as.tokens(tsm_u$斷詞)
tsm_u <- sentence_tsm_tagged %>%
filter(str_detect(斷詞, '\\b有/')) %>%
select(-例句)
tsm_u %>% head(10) %>% knitr::kable(align = 'c')
tsm_u_corpus <- corpus(tsm_u,
docid_field = '編號',
text_field = '斷詞')
corpus_tsm_u <- corpus(tsm_u,
docid_field = '編號',
text_field = '斷詞')
kwic(corpus_tsm_u, '有/V') %>% head(10) %>% knitr::kable(align = 'c')
tsm_u <- sentence_tsm_tagged %>%
filter(str_detect(斷詞, '\\b有/')) %>%
select(c(1, 5, 3, 4))
tsm_u <- sentence_tsm_tagged %>%
filter(str_detect(斷詞, '\\b有/')) %>%
select(c(1, 5, 3, 4))
tsm_u %>% head(10) %>% knitr::kable(align = 'c')
corpus_tsm_u <- corpus(tsm_u,
docid_field = '編號',
text_field = '斷詞')
kwic(corpus_tsm_u, '有/V') %>% head(10) %>% knitr::kable(align = 'c')
kwic(corpus_tsm_u, '有/v') %>% head(10) %>% knitr::kable(align = 'c')
tagger <- worker(type = "tag",
user = "TSM_dict.txt",
symbol = T,
bylines = F)
segmenter <- worker(user = "TSM_dict.txt",
symbol = T,
bylines = F)
# define own function
tag_text <- function(x, jiebar){
segment(x, jiebar) %>%
paste(names(.), sep = "/", collapse = " ")
}
seg_text <- function(x, jiebar){
segment(x, jiebar) %>%
paste(collapse = " ")
}
sentence_tsm_tagged <- sentence_tsm %>%
mutate(斷詞 = map_chr(例句, seg_text, segmenter),
斷詞標記 = map_chr(例句, tag_text, tagger))
tsm_u <- sentence_tsm_tagged %>%
filter(str_detect(斷詞, '\\b有/'))
tsm_u %>% head(10) %>% knitr::kable(align = 'c')
sentence_tsm_tagged
tsm_u <- sentence_tsm_tagged %>%
filter(str_detect(斷詞標記, '\\b有/'))
tsm_u %>% head(10) %>% knitr::kable(align = 'c')
corpus_tsm_u <- corpus(tsm_u,
docid_field = '編號',
text_field = '斷詞')
kwic(corpus_tsm_u, '有/v') %>% head(10) %>% knitr::kable(align = 'c')
kwic(corpus_tsm_u, '有') %>% head(10) %>% knitr::kable(align = 'c')
corpus_tsm_u <- corpus(tsm_u,
docid_field = '編號',
text_field = '斷詞標記')
kwic(corpus_tsm_u, '有/v') %>% head(10) %>% knitr::kable(align = 'c')
kwic(corpus_tsm_u, '有/v') %>% head(10) %>% knitr::kable(align = 'c')
corpus_tsm_u <- corpus(tsm_u,
docid_field = '編號',
text_field = '斷詞')
kwic(corpus_tsm_u, '有') %>% head(10) %>% knitr::kable(align = 'c')
kwic(corpus_tsm_u, '有') %>%
head(50) %>%
knitr::kable(align = 'c')
kwic(corpus_tsm_u, '有') %>%
mutate(context = tsm_u$斷詞標記[match(docname, tsm_u$編號)]) %>%
select(-from, -to, -pattern) %>%
head(50) %>%
knitr::kable(align = 'c')
knitr::opts_chunk$set(echo = TRUE)
# define chunk tokenization function
tokenizer_chunks <- function(input){
str_split(input, pattern = "[，。？！]")
}
# convert text to pattern
tsm_u %>%
select(編號, 斷詞標記) %>%
unnest_tokens(chunk, 斷詞標記,
token = tokenizer_chunks) %>%
filter(nzchar(chunk)) -> apple_chunk
library(jiebaR)
library(dplyr)
library(readr)
library(stringr)
library(tidyverse)
library(quanteda)
library(tidytext)
# convert text to pattern
tsm_u %>%
select(編號, 斷詞標記) %>%
unnest_tokens(chunk, 斷詞標記,
token = tokenizer_chunks) %>%
filter(nzchar(chunk)) -> apple_chunk
tsm_u %>%
select(編號, 斷詞標記) %>%
unnest_tokens(chunk, 斷詞標記,
token = tokenizer_chunks)
tsm_u %>%
select(編號, 斷詞標記) %>%
unnest_tokens(chunk, 斷詞標記,
token = tokenizer_chunks) %>%
filter(chunk != '/x') %>%
unnest_tokens(pattern, chunck,
token = function(x) str_extract_all(x, '\\b有/v\\s(\\w+/[^n]+)*?\\w+/[na]\\w*'))
tsm_u %>%
select(編號, 斷詞標記) %>%
unnest_tokens(chunk, 斷詞標記,
token = tokenizer_chunks) %>%
filter(chunk != '/x') %>%
unnest_tokens(pattern, chunck,
token = function(x) str_extract_all(x, '\\b有/v\\s(\\w+/[^n]+)*?\\w+/[va]\\w*'))
tsm_u %>%
select(編號, 斷詞標記) %>%
unnest_tokens(chunk, 斷詞標記,
token = tokenizer_chunks) %>%
filter(chunk != '/x')
tsm_u %>%
select(編號, 斷詞) %>%
unnest_tokens(chunk, 斷詞,
token = tokenizer_chunks) %>%
filter(nzchar(chunk))
tokenizer_chunks <- function(input){
str_split(input, pattern = "[(，/x)(。/x)(？/x)(！/x)]")
}
tsm_u %>%
select(編號, 斷詞標記) %>%
unnest_tokens(chunk, 斷詞標記,
token = tokenizer_chunks)
tokenizer_chunks <- function(input){
str_split(input, pattern = "[(，/x)|(。/x)|(？/x)|(！/x)]")
}
tsm_u %>%
select(編號, 斷詞標記) %>%
unnest_tokens(chunk, 斷詞標記,
token = tokenizer_chunks)
tokenizer_chunks <- function(input){
str_split(input, pattern = "(，/x)|(。/x)|(？/x)|(！/x)")
}
tsm_u %>%
select(編號, 斷詞標記) %>%
unnest_tokens(chunk, 斷詞標記,
token = tokenizer_chunks)
tsm_u %>%
select(編號, 斷詞標記) %>%
unnest_tokens(chunk, 斷詞標記,
token = tokenizer_chunks) %>%
filter(nzchar(chunk))
tsm_u %>%
select(編號, 斷詞標記) %>%
unnest_tokens(chunk, 斷詞標記,
token = tokenizer_chunks) %>%
filter(nzchar(chunk)) %>%
unnest_tokens(pattern, chunck,
token = function(x) str_extract_all(x, '\\b有/v\\s(\\w+/[^n]+)*?\\w+/[va]\\w*'))
tsm_u %>%
select(編號, 斷詞標記) %>%
unnest_tokens(chunk, 斷詞標記,
token = tokenizer_chunks) %>%
filter(nzchar(chunk)) %>%
unnest_tokens(pattern, chunck,
token = function(x) str_extract_all(x, '\\b有/v\\s([^/]+/[^n]+)*?\\w+/[va]\\w*'))
tsm_u %>%
select(編號, 斷詞標記) %>%
unnest_tokens(chunk, 斷詞標記,
token = tokenizer_chunks) %>%
filter(nzchar(chunk)) %>%
unnest_tokens(pattern, chunck,
token = function(x) str_extract_all(x, '\\b有/v\\s([^/]+/[^n]+)*?\\w+/[va]'))
tsm_u %>%
select(編號, 斷詞標記) %>%
unnest_tokens(chunk, 斷詞標記,
token = tokenizer_chunks) %>%
filter(nzchar(chunk))
tsm_u %>%
select(編號, 斷詞標記) %>%
unnest_tokens(chunk, 斷詞標記,
token = tokenizer_chunks) %>%
filter(nzchar(chunk)) %>%
unnest_tokens(pattern, chunck,
token = function(x) str_extract_all(x, '\\b有/v\\s([^/]+/[^\\sn]+)*?\\w+/[va]'))
tsm_u %>%
select(編號, 斷詞標記) %>%
unnest_tokens(chunk, 斷詞標記,
token = tokenizer_chunks) %>%
filter(nzchar(chunk))
tsm_u %>%
select(編號, 斷詞標記) %>%
unnest_tokens(chunk, 斷詞標記,
token = tokenizer_chunks) %>%
filter(nzchar(chunk)) %>%
unnest_tokens(pattern, chunck,
token = function(x) str_extract_all(x, '\\b有/v.*?/[va]'))
tsm_u %>%
select(編號, 斷詞標記) %>%
unnest_tokens(chunk, 斷詞標記,
token = tokenizer_chunks) %>%
filter(nzchar(chunk)) %>%
unnest_tokens(pattern, chunck,
token = function(x) str_extract_all(x, '\\b有/v.*?/[va]'))
tsm_u %>%
select(編號, 斷詞標記) %>%
unnest_tokens(chunk, 斷詞標記,
token = tokenizer_chunks) %>%
filter(nzchar(chunk))
tsm_u %>%
select(編號, 斷詞標記) %>%
unnest_tokens(chunk, 斷詞標記,
token = tokenizer_chunks) %>%
filter(nzchar(chunk)) %>%
unnest_tokens(pattern, chunck,
token = function(x) str_extract_all(x, '有/v.*?/[va]'))
tsm_u %>%
select(編號, 斷詞標記) %>%
unnest_tokens(chunk, 斷詞標記,
token = tokenizer_chunks) %>%
filter(nzchar(chunk)) %>%
unnest_tokens(pattern, chunck,
token = function(x) str_extract_all(x, '\\b有/v.+?/[va]'))
tsm_u %>%
select(編號, 斷詞標記) %>%
unnest_tokens(chunk, 斷詞標記,
token = tokenizer_chunks) %>%
filter(nzchar(chunk)) %>%
unnest_tokens(pattern, chunck,
token = function(x) str_extract_all(x, '\\b有/v.+?/v'))
tsm_u %>%
select(編號, 斷詞標記) %>%
unnest_tokens(chunk, 斷詞標記,
token = tokenizer_chunks) %>%
filter(nzchar(chunk)) %>%
unnest_tokens(pattern, chunck,
token = function(x) str_extract_all(x, '\\b有/v\\s\\w+/v'))
tsm_u %>%
select(編號, 斷詞標記) %>%
unnest_tokens(chunk, 斷詞標記,
token = tokenizer_chunks) %>%
filter(nzchar(chunk)) %>%
unnest_tokens(pattern, chunck,
token = function(x) str_extract_all(x, '\\b有/v\\s((?!/n).)*?\\w+/(a|v)'))
tsm_u %>%
select(編號, 斷詞標記) %>%
unnest_tokens(chunk, 斷詞標記,
token = tokenizer_chunks) %>%
filter(nzchar(chunk)) %>%
unnest_tokens(pattern, chunck,
token = function(x) str_extract_all(x, '\\b有/v\\s((?!/n).)*\\w+/(a|v)'))
tsm_u %>%
select(編號, 斷詞標記) %>%
unnest_tokens(chunk, 斷詞標記,
token = tokenizer_chunks) %>%
filter(nzchar(chunk)) %>%
unnest_tokens(pattern, chunck,
token = function(x) str_extract_all(x, '\\b有/v\\s((?!/n).)*\\s\\w+/(a|v)'))
tsm_u %>%
select(編號, 斷詞標記) %>%
unnest_tokens(chunk, 斷詞標記,
token = tokenizer_chunks) %>%
filter(nzchar(chunk)) %>%
unnest_tokens(pattern, chunck,
token = function(x) str_extract_all(x, '有/v\\s((?!/n).)*\\s\\w+/(a|v)'))
tsm_u %>%
select(編號, 斷詞標記) %>%
unnest_tokens(chunk, 斷詞標記,
token = tokenizer_chunks) %>%
filter(nzchar(chunk)) %>%
unnest_tokens(pattern, chunck,
token = function(x) str_extract_all(x, '有/v\\s((?!/n).)*?\\s\\w+/(a|v)'))
tsm_u %>%
select(編號, 斷詞標記) %>%
unnest_tokens(chunk, 斷詞標記,
token = tokenizer_chunks) %>%
filter(nzchar(chunk)) %>%
unnest_tokens(pattern, chunck,
token = function(x) str_extract_all(x, '有/v\\s((?!/n\\b).)*?\\s\\w+/(a|v)'))
tsm_u %>%
select(編號, 斷詞標記) %>%
unnest_tokens(chunk, 斷詞標記,
token = tokenizer_chunks) %>%
filter(nzchar(chunk)) %>%
unnest_tokens(pattern, chunck,
token = function(x) str_extract_all(x, '有/v\\s((?!/n\\b).)*?\\w+/(a|v)'))
tsm_u %>%
select(編號, 斷詞標記) %>%
unnest_tokens(chunk, 斷詞標記,
token = tokenizer_chunks) %>%
filter(nzchar(chunk)) %>%
unnest_tokens(pattern, chunck,
token = function(x) str_extract_all(x, '有/v\\s((?!/n\\b).)*\\w+/(a|v)'))
tsm_u %>%
select(編號, 斷詞標記) %>%
unnest_tokens(chunk, 斷詞標記,
token = tokenizer_chunks) %>%
filter(nzchar(chunk)) %>%
unnest_tokens(pattern, chunck,
token = function(x) str_extract_all(x, '\\b有/v\\s((?!/n\\b).)*\\w+/(a|v)'))
tsm_u %>%
select(編號, 斷詞標記) %>%
unnest_tokens(chunk, 斷詞標記,
token = tokenizer_chunks) %>%
filter(nzchar(chunk))
tsm_u %>%
select(編號, 斷詞標記) %>%
unnest_tokens(chunk, 斷詞標記,
token = tokenizer_chunks) %>%
filter(nzchar(chunk)) %>%
unnest_tokens(pattern, chunck,
token = function(x) str_extract_all(x, '\\b有/v'))
# convert text to pattern
tsm_u %>%
select(編號, 斷詞標記) %>%
unnest_tokens(chunk, 斷詞標記,
token = tokenizer_chunks) %>%
filter(nzchar(chunk)) %>%
unnest_tokens(pattern, chunck,
token = function(x) str_extract_all(x, '有/v'))
tsm_u %>%
select(編號, 斷詞標記) %>%
unnest_tokens(chunk, 斷詞標記,
token = tokenizer_chunks) %>%
filter(nzchar(chunk)) %>%
unnest_tokens(pattern, chunck,
token = function(x) str_extract_all(x, '有/V'))
tsm_u %>%
select(編號, 斷詞標記) %>%
unnest_tokens(chunk, 斷詞標記,
token = tokenizer_chunks) %>%
filter(nzchar(chunk))
tsm_u %>%
select(編號, 斷詞標記) %>%
unnest_tokens(chunk, 斷詞標記,
token = tokenizer_chunks) %>%
filter(nzchar(chunk)) %>%
unnest_tokens(pattern, chunck,
token = function(x) str_extract_all(x, "\\b有/v\\s([^/]+/[^\\s]+\\s)*?[^/]+/v"))
tsm_u %>%
select(編號, 斷詞標記) %>%
unnest_tokens(pattern, chunck,
token = function(x) str_extract_all(x, "\\b有/v\\s([^/]+/[^\\s]+\\s)*?[^/]+/v"))
tsm_u %>%
select(編號, 斷詞標記) %>%
unnest_tokens(pattern, 斷詞標記,
token = function(x) str_extract_all(x, "\\b有/v\\s([^/]+/[^\\s]+\\s)*?[^/]+/v"))
tsm_u %>%
select(編號, 斷詞標記) %>%
unnest_tokens(chunk, 斷詞標記,
token = tokenizer_chunks) %>%
filter(nzchar(chunk)) %>%
unnest_tokens(pattern, chunck,
token = function(x) str_extract_all(x, "\\b有/v\\s([^/]+/[^\\s]+\\s)*?[^/]+/v"))
tsm_u %>%
select(編號, 斷詞標記) %>%
unnest_tokens(chunk, 斷詞標記,
token = tokenizer_chunks) %>%
#filter(nzchar(chunk)) %>%
unnest_tokens(pattern, chunck,
token = function(x) str_extract_all(x, "\\b有/v\\s([^/]+/[^\\s]+\\s)*?[^/]+/v"))
tsm_u %>%
select(編號, 斷詞標記) %>%
unnest_tokens(chunk, 斷詞標記,
token = tokenizer_chunks) %>%
#filter(nzchar(chunk)) %>%
unnest_tokens(pattern, chunck,
token = function(x) str_extract(x, "\\b有/v\\s([^/]+/[^\\s]+\\s)*?[^/]+/v"))
tsm_u %>%
select(編號, 斷詞標記) %>%
unnest_tokens(chunk, 斷詞標記,
token = tokenizer_chunks) %>%
filter(nzchar(chunk))
tsm_u %>%
select(編號, 斷詞標記) %>%
unnest_tokens(chunk, 斷詞標記,
token = tokenizer_chunks) %>%
filter(nzchar(chunk)) %>%
unnest_tokens(pattern, chunck,
token = function(x) str_extract(x, "\\b有/v\\s(\\w+/[^n\\s]+\\s)*?\\w+/v"))
tsm_u %>%
select(編號, 斷詞標記) %>%
unnest_tokens(chunk, 斷詞標記,
token = tokenizer_chunks) %>%
filter(nzchar(chunk)) %>%
unnest_tokens(pattern, chunck,
token = function(x) str_extract(x, "\\b有/v\\s(\\w+/[^n\\s]+\\s)*?\\w+/(a|v)"))
library(readxl)
library(officer)
tsm_u
tsm_u[1,]
setwd("C:/Users/Mao-Chang/Desktop/TSM u")
doc <- read_docx()
for (i in 1:nrow(tsm_u) {
for (i in 1:nrow(tsm_u)) {
doc %>%
body_add_par(tsm_u[i,]$標音) %>%
body_add_par(tsm_u[i,]$華語翻譯) %>%
body_add_par('') %>%
}
for (i in 1:nrow(tsm_u)) {
doc %>%
body_add_par(tsm_u[i,]$標音) %>%
body_add_par(tsm_u[i,]$華語翻譯) %>%
body_add_par('')
}
for (i in 1:nrow(tsm_u)) {
doc %>%
body_add_par(tsm_u[i,]$標音) %>%
body_add_par(tsm_u[i,]$華語翻譯) %>%
body_add_par('')
}
doc <- read_docx()
for (i in 1:nrow(tsm_u)) {
doc %>%
body_add_par(tsm_u[i,]$標音) %>%
body_add_par(tsm_u[i,]$華語翻譯) %>%
body_add_par('')
print(doc, target = 'TSM_u.docx'))
print(doc, target = 'TSM_u.docx')
doc <- read_docx()
for (i in 1:nrow(tsm_u)) {
doc %>%
body_add_par(tsm_u[i,]$標音) %>%
body_add_par(tsm_u[i,]$華語翻譯) %>%
body_add_par('')
}
print(doc, target = 'TSM_u.docx')
install.packages('flextable')
library(flextable)
doc <- read_docx()
for (i in 1:nrow(tsm_u)) {
doc %>%
body_add_par(tsm_u[i,]$標音) %>%
body_add_par(tsm_u[i,]$華語翻譯) %>%
body_add_par('')
}
qflextable(head(tsm_u)) %>%
body_add_flextable()
df <- qflextable(head(tsm_u))
body_add_flextable(doc, df)
print(doc, target = 'TSM_u.docx')
doc <- read_docx()
for (i in 1:nrow(tsm_u)) {
doc %>%
body_add_par(tsm_u[i,]$標音) %>%
body_add_par(tsm_u[i,]$華語翻譯) %>%
body_add_par('')
}
df <- qflextable(head(sentence_tsm))
body_add_flextable(doc, df)
print(doc, target = 'TSM_u.docx')
